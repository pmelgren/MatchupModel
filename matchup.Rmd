---
title: "XGBoost Matchup Model"
author: "Pete Melgren"
date: "August 21, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(baseballr) # installed from github.com/BillPetti/baseballr
library(data.table) # use data.table package to optimize speed
library(ggplot2)
library(Matrix)
library(xgboost)
```

## Data Preparation

Begin by bringing in the event data from the publicly available retrosheet database. Instructions on bringing retrosheet into an SQL table can be found [here](github.com/pmelgren/baseballdatabase) or [here](https://tht.fangraphs.com/databases-for-sabermetricians-part-one/).

Next import the statcast data and aggregate relevant variables by player and pitch type.
```{r scrape data, cache = TRUE}
#import statcast data in multiple chunks to prevent timeout. This will take a few minutes.
if(!exists("sc")){ # use this if statement to prevent unnecessarily scraping the same data multiple times so as to not abuse the site.
  sc = data.frame()
  start = as.Date("2018-03-29")
  end = start+6
  while(start < as.Date("2018-10-01")){
    sc = rbind(sc,scrape_statcast_savant(start,end))
    start = start + 7
    end = end + 7
  }
}

# set sc to a data.table to optimize for speed
sc = data.table(sc)

# filter out only regular season
sc = sc[game_type == "R"]

# add binary indicators for if the pitch was in-zone and if the batter swung
sc[,inzone := ifelse(abs(plate_x)<.75 & plate_z<sz_top & plate_z>sz_bot, 1, 0)]
sc[grep("hit_in?|swing?|foul|foul?|?bunt",description),swing := 1]
sc[is.na(swing), swing := 0]
sc[grep("?ball|hit_by_pitch|pitchout",description),strike := 0]
sc[is.na(strike), strike := 1]
```

```{r aggregate pitchers}
# aggregate pitcher data into a few key metrics
p_agg = sc[,list(P.pit = length(batter)
                 ,wOBA.pit = sum(woba_value,na.rm = TRUE) /
                   sum(woba_denom,na.rm = TRUE)
                 ,FF_pct = mean(pitch_type == "FF",na.rm = TRUE)
                 ,SI_pct = mean(pitch_type == "SI",na.rm = TRUE)
                 ,SL_pct = mean(pitch_type == "SL",na.rm = TRUE)
                 ,CH_pct = mean(pitch_type %in% c("CH","FS"),na.rm = TRUE)
                 ,CB_pct = mean(pitch_type %in% c("CU","KC"),na.rm = TRUE)
                 ,FB_Velo = mean(ifelse(pitch_type %in% c("FF","SI")
                                        ,release_speed,NA),na.rm = TRUE)
                 ,SL_Velo = mean(ifelse(pitch_type == "SL"
                                        ,release_speed,NA),na.rm = TRUE)
                 ,SL_Spin = mean(ifelse(pitch_type == "SL"
                                        ,release_spin_rate,NA),na.rm = TRUE)
                 ,CB_Spin = mean(ifelse(pitch_type %in% c("CU","KC")
                                        ,release_spin_rate,NA),na.rm = TRUE)
                 ,Extension = mean(release_extension,na.rm =TRUE)
                 ,LA.pit = mean(launch_angle,na.rm = TRUE)
                 ,Strike_pct = mean(strike,na.rm = TRUE)
                 ,Zone_pct = mean(zone,na.rm = TRUE))
           ,by = pitcher]
str(p_agg)
summary(p_agg)
```

```{r aggregate batters}
# aggregate batter data
b_agg = sc[,list(P.bat = length(pitcher)
                 ,wOBA.bat = sum(woba_value,na.rm = TRUE) / 
                   sum(woba_denom,na.rm = TRUE)
                 ,Exit_velo = mean(launch_speed,na.rm = TRUE)
                 ,LA.bat = mean(launch_angle,na.rm = TRUE)
                 ,Swing_pct = sum(swing)/length(pitcher)
                 ,Whiff = sum(description == "swinging_strike")/sum(swing)
                 ,Chase = sum(swing*(1-inzone),na.rm = TRUE)/length(pitcher)
                 ,Zcontact = (sum(inzone*swing*(description != "swinging_strike")
                                  ,na.rm = TRUE)/sum(inzone*swing,na.rm = TRUE)))
           ,by = batter]
str(b_agg)
summary(b_agg)
```

```{r event data}
events = sc[woba_denom == 1
            ,list(pitcher,batter,inning
                  ,outs = outs_when_up
                  ,b_side = stand
                  ,p_throws
                  ,score_diff = home_score-away_score
                  ,base_state = as.factor(paste0(ifelse(is.na(on_1b),"1","_")
                                                 ,ifelse(is.na(on_2b),"2","_")
                                                 ,ifelse(is.na(on_3b),"3","_")))
                  ,woba_value
                  
                  )
            ]
str(events)
summary(events)
```

With our 3 tables in place, we can now combine them into one table that we'll use to train the model.
```{r join datasets}
model_dat = merge(events,p_agg,by = c("pitcher"))
model_dat = merge(model_dat,b_agg,by = c("batter"))

str(model_dat)
summary(model_dat)
```

Right away we notice that the pitcher and batter id's are treated as numeric data when they should be categorical. But before we do that conversion, we also need to account for the pitchers and batters that have a really small sample size. So what we'll do is mask the ID of any pitcher or batter that doesn't meet a certain pitch threshold. So let's explore what that threshold should be.

We'll begin by plotting density of pitchers and batters by pitch_count to try to get a good idea of what a reasonable masking threshold might be.
```{r masking eploration plot}
plot_dat = data.table(P = p_agg$P.pit, Position = "Pitcher")
plot_dat = rbind(plot_dat,data.table(P= b_agg$P.bat, Position = "Batter"))
ggplot(plot_dat,aes(x = P, color = Position)) + 
  stat_ecdf(size = 1) + 
  labs(y = "Cumulative Percentage"
       ,title = "Cumulative Percentage of Players by Pitch Threshold")
rm(plot_dat)
```

Based on this, it looks like 200 pitches is a reasonable cutoff to mask the ID's of both pitchers and batters. Although it will mask more batters, there are more total batters in the sample. Just to verify this Let's look at how many players will remain unmasked for each catergory.
```{r unmasked numbers}
print(paste("Pitchers:",nrow(p_agg)-nrow(p_agg[P.pit < 200])," of ",nrow(p_agg)))
print(paste("Batters:",nrow(b_agg)-nrow(b_agg[P.bat < 200])," of ",nrow(b_agg)))
```

We lose a high percentage of batters, but still have a large number of players in each category, so we'll stick with 200. Then once we've masked these players we can go ahead and convert the variables to categrical.
```{r masking}
# mask pitcher and batter ID's with fewer than 200 pitches
model_dat[P.pit < 200, pitcher := 0]
model_dat[P.bat < 200, batter := 0]

# convert to factors to indicate they're categorical variables
model_dat[,pitcher := as.factor(as.character(pitcher))]
model_dat[,batter := as.factor(as.character(batter))]

str(model_dat)
```

Now that our data is ready, we can prepare it for our model by converting to a matrix. We're going to use a sparse matrix given the high number of categorical columns in our data.

```{r convert to matrix}
options(na.action='na.pass')
train_dat = sparse.model.matrix(woba_value~.,data = model_dat)
dim(train_dat)
```

Now we'll set up an initial grid search. R doesn't have good support of grid search functionality for sparse model matrixes so we'll do it manually using a for loop.

The first thing we want to determine is the optimum tree depth (interactions). So we'll begin by searching over tree depths of various lengths with a high learning rate.

```{r first grid search, cache=TRUE}
#create a data frame to store the output of the tuning exercies
tune_results = data.frame()
tunecv = list()
rnum = 1

# Set the preliminary tuning parameters
d = seq(1,7,2)
lr = c(.1)

# begin by identifying the best tree depth
for(i in 1:length(d)){
  for(j in 1:length(lr)){
    
    print(paste("Depth of",d[i],"and learning rate of",lr[j]))
    
    # calculate cross-validated tuning metrics
    tunecv[[rnum]] = xgb.cv(data = train_dat
                            ,label = model_dat$woba_value
                            ,max.depth = d[i]
                            ,eta = lr[j]
                            ,nround = 5000
                            ,objective = "reg:squarederror"
                            ,nfold = 5
                            ,early_stopping_rounds = 100
                            ,verbose = TRUE
                            ,print_every_n = 100
                            )
    
    #add data to the output variable
    tune_results = rbind(tune_results
      ,data.frame(depth = d[i]
                  ,lr = lr[j]
                  ,min_err = min(tunecv[[rnum]]$evaluation_log$test_rmse_mean)
                  ,best_iter = tunecv[[rnum]]$best_iter))
    rnum = rnum+1
  }
}
```
```{r plot first grid search}
plotdat = data.table()
for(i in 1:length(tunecv)){
  plotdat = rbind(plotdat
                  ,data.table(test_rmse = tunecv[[i]]$evaluation_log$test_rmse_mean
                              ,params = paste0("d:",tunecv[[i]]$params$max_depth
                                               ,"_lr:",tunecv[[i]]$params$eta)
                              ,iter = tunecv[[i]]$evaluation_log$iter
                              ,features = "all"))
}
ggplot(plotdat,aes(iter,test_rmse,color = params))+ geom_line() + 
  coord_cartesian(ylim = c(.511,.52))

plotdat[test_rmse == min(test_rmse)]
```

We'll refine the grid search now by eliminating the larger tree depths that didn't perform well and trying lower learning rates.

```{r refined grid search, cache = TRUE}
# Set the preliminary tuning parameters
d = 1:3
lr = c(.05,.025,.01)

# begin by identifying the best tree depth
for(i in 1:length(d)){
  for(j in 1:length(lr)){
    
    print(paste("Depth of",d[i],"and learning rate of",lr[j]))
    
    # calculate cross-validated tuning metrics
    tunecv[[rnum]] = xgb.cv(data = train_dat
                            ,label = model_dat$woba_value
                            ,max.depth = d[i]
                            ,eta = lr[j]
                            ,nround = 5000
                            ,objective = "reg:squarederror"
                            ,nfold = 5
                            ,early_stopping_rounds = 100
                            ,verbose = TRUE
                            ,print_every_n = 100
                            )
    
    #add data to the output variable
    tune_results = rbind(tune_results
      ,data.frame(depth = d[i]
                  ,lr = lr[j]
                  ,min_err = min(tunecv[[rnum]]$evaluation_log$test_rmse_mean)
                  ,best_iter = tunecv[[rnum]]$best_iter
                  ,features = "all"))
    rnum = rnum+1
  }
}
```
```{r plot refined grid search}
plotdat = data.table()
for(i in 1:length(tunecv)){
  if(tunecv[[i]]$params$max_depth <= 3){
      plotdat = rbind(plotdat
                  ,data.table(test_rmse = tunecv[[i]]$evaluation_log$test_rmse_mean
                              ,params = paste0("d:",tunecv[[i]]$params$max_depth
                                               ,"_lr:",tunecv[[i]]$params$eta)
                              ,iter = tunecv[[i]]$evaluation_log$iter))
  }
}
ggplot(plotdat,aes(iter,test_rmse,color = params))+geom_line() + 
  coord_cartesian(ylim = c(.511,.512))

best = which(tune_results$min_err == min(tune_results$min_err))
tune_results[best,]
```

```{r train model}
model.xgb = xgboost(data = train_dat
                    ,label = model_dat$woba_value
                    ,max.depth = tune_results$depth[best]
                    ,eta = tune_results$lr[best]
                    ,nround = tune_results$best_iter[best]
                    ,objective = "reg:squarederror"
                    ,metric = "rmse"
                    ,verbose = TRUE
                    ,print_every_n = 50)
```

```{r feature importance}
imp = xgb.importance(train_dat@Dimnames[[2]], model = model.xgb)
imp
```
It stands out that a lot of features don't appear in the model, so we're going to remove all but the pitcher/batter woba and game situational factors as those seem to matter the most here.

We expect the same basic tuning parameters to optimize this model as well, but we will go ahead and re-tune using this new dataset just to make sure.
```{r train data for simple feature set, cache = TRUE}
train_dat_list = list(train_dat)

# convert to a sparse model matrix using the higher threshold
train_dat_list[[2]]= model.matrix(woba_value~wOBA.pit+wOBA.bat+outs+inning+
                                    base_state+score_diff
                                         ,data = model_dat)
print(dim(train_dat_list[[2]]))


rnum = length(tunecv)+1
d = 1:5
lr = c(.1,.05,.025)

for(i in 1:length(d)){
  for(j in 1:length(lr)){
    
    print(paste("Depth of",d[i],"and learning rate of",lr[j]))

    tunecv[[rnum]] = xgb.cv(data = train_dat_list[[2]]
                            ,label = model_dat$woba_value
                            ,max.depth = d[i]
                            ,eta = lr[j]
                            ,nround = 5000
                            ,objective = "reg:squarederror"
                            ,nfold = 5
                            ,early_stopping_rounds = 100
                            ,verbose = TRUE
                            ,print_every_n = 100
                            )
    tune_results = rbind(tune_results
      ,data.frame(depth = d[i]
                  ,lr = lr[j]
                  ,min_err = min(tunecv[[rnum]]$evaluation_log$test_rmse_mean)
                  ,best_iter = tunecv[[rnum]]$best_iter
                  ,features = "wOBA/situational"
                  )
      )
    rnum = rnum+1  
  }
}
```
```{r plot grid search of simple feature set}
plotdat2 = data.table()
for(i in 1:length(tunecv)){
  if(tune_results$features == "wOBA/situational"){
      plotdat2 = rbind(plotdat2
        ,data.table(test_rmse = tunecv[[i]]$evaluation_log$test_rmse_mean
                    ,params = paste0("d:",tunecv[[i]]$params$max_depth
                                               ,"_lr:",tunecv[[i]]$params$eta)
                    ,iter = tunecv[[i]]$evaluation_log$iter))
  }
}
ggplot(plotdat2,aes(iter,test_rmse, color = params)) +
  geom_line() + coord_cartesian(ylim = c(.51,.52))

print("Best iteration from new thresholds:")
print.data.frame(plotdat2[test_rmse == min(test_rmse)])
print("Best iteration from original threshold:")
print.data.frame(plotdat[test_rmse == min(test_rmse)])

best2 = which(tune_results$min_err == min(tune_results$min_err))
```

```{r specify 2nd model}
if(best2 != best){
  orig_model = model.xgb # preserve old model
  model.xgb = xgboost(data = train_dat_list[[2]]
                      ,label = model_dat$woba_value
                      ,max.depth = tune_results$depth[best2]
                      ,eta = tune_results$lr[best2]
                      ,nround = tune_results$best_iter[best2]
                      ,objective = "reg:squarederror"
                      ,metric = "rmse"
                      ,verbose = TRUE
                      ,print_every_n = 50) # tune new model
}
```

```{r feature importance #2}
imp = xgb.importance(colnames(train_dat_list[[2]]), model = model.xgb)
imp
```

Since this model does use different parameters, including a different tree depth, we'll try introducing a few other variables now. Specifically, I'm still not convinced handedness doesn't matter, so we'll introduce that to see if it sticks.
```{r grid search handedness back, cache = TRUE}

# convert to a sparse model matrix using the higher threshold
train_dat_list[[3]]= model.matrix(woba_value~wOBA.pit+wOBA.bat+outs+inning+
                                    base_state+score_diff+p_throws+b_side
                                         ,data = model_dat)
print(dim(train_dat_list[[3]]))


rnum = length(tunecv)+1
d = 1:3
lr = c(.1,.05,.025)

for(i in 1:length(d)){
  for(j in 1:length(lr)){
    
    print(paste("Depth of",d[i],"and learning rate of",lr[j]))

    tunecv[[rnum]] = xgb.cv(data = train_dat_list[[2]]
                            ,label = model_dat$woba_value
                            ,max.depth = d[i]
                            ,eta = lr[j]
                            ,nround = 5000
                            ,objective = "reg:squarederror"
                            ,nfold = 5
                            ,early_stopping_rounds = 100
                            ,verbose = TRUE
                            ,print_every_n = 100
                            )
    tune_results = rbind(tune_results
      ,data.frame(depth = d[i]
                  ,lr = lr[j]
                  ,min_err = min(tunecv[[rnum]]$evaluation_log$test_rmse_mean)
                  ,best_iter = tunecv[[rnum]]$best_iter
                  ,features = "wOBA/situational/handedness"
                  )
      )
    rnum = rnum+1  
  }
}
```

```{r plot grid search with handedness back}
plotdat3 = data.table()
for(i in 1:length(tunecv)){
  if(tune_results$features == "wOBA/situational/handedness"){
      plotdat3 = rbind(plotdat3
        ,data.table(test_rmse = tunecv[[i]]$evaluation_log$test_rmse_mean
                    ,params = paste0("d:",tunecv[[i]]$params$max_depth
                                               ,"_lr:",tunecv[[i]]$params$eta)
                    ,iter = tunecv[[i]]$evaluation_log$iter))
  }
}
ggplot(plotdat3,aes(iter,test_rmse, color = params)) +
  geom_line() + coord_cartesian(ylim = c(.51,.52))

print("Best iteration with handedness features:")
print.data.frame(plotdat3[test_rmse == min(test_rmse)])
print("Best iteration without handedness features:")
print.data.frame(plotdat2[test_rmse == min(test_rmse)])

best3 = which(tune_results$min_err == min(tune_results$min_err))
```

```{r specify 3rd model}
if(best2 != best){
  models = list(all_features = orig_model
                ,no_id_or_handedness = model.xgb) # preserve old model
  models$handedness = xgboost(data = train_dat_list[[3]]
                              ,label = model_dat$woba_value
                              ,max.depth = tune_results$depth[best3]
                              ,eta = tune_results$lr[best3]
                              ,nround = tune_results$best_iter[best3]
                              ,objective = "reg:squarederror"
                              ,metric = "rmse"
                              ,verbose = TRUE
                              ,print_every_n = 50) # tune new model
}
```

```{r feature importance #3}
imp = xgb.importance(colnames(train_dat_list[[3]]), model = models$handedness)
imp
```

At this point we could continue to fine-tune this model, but gains will remain minimal and we have a model that uses a simple but intuitive feature set so we will save this model for future use.

```{r save model}
xgb.save(models$handedness,"Matchup.model")
```

